### 整体调用链先说清楚

**顶层流程**（无论是 `tavily` 还是 `volc_bot`）：

- **1. 在入口里选一个搜索引擎实例**
  - `run_local.py` / `server.py` 里：  
    - 默认用 `VolcBotSearchEngine(bot_id=SEARCH_BOT_ID)`  
    - 如果 `SEARCH_ENGINE == "tavily"`，则用 `TavilySearchEngine(api_key=TAVILY_API_KEY)`
- **2. 把这个 `search_engine` 注入到 `DeepSearch`/`DeepResearch`**
  - `DeepSearch(search_engine=search_engine, planning_endpoint_id=..., summary_endpoint_id=...)`
- **3. DeepSearch 在「规划阶段」里调用 `search_engine.asearch(...)`**
  - `deep_search.py` 中，在 `astream_planning` 里：
    - 让 LLM 先「规划」要查的关键词 `new_queries`
    - 然后执行：`search_results = await self.search_engine.asearch(new_queries)`
    - 再把结果塞进 `ResultsSummary`，并作为 `reference` 字符串传回给 LLM（规划/总结模型）

### SearchEngine 抽象接口 & SearchResult 结构

`search_engine/search_engine.py` 定义了统一接口和结果结构：

- **`SearchEngine` 接口（你要实现的最小要求）**
  - `def search(self, queries: List[str]) -> List[SearchResult]: ...`
  - `async def asearch(self, queries: List[str]) -> List[SearchResult]: ...`
- **`SearchResult`**：LLM 最终看到的「搜索结果文本」是这里：
  - `query: str`：这次搜索对应的查询词
  - `summary_content: Optional[str]`：**给 LLM 用的、已经整理成纯文本的摘要**（DeepSearch 会把所有 `summary_content` 拼起来给 LLM 当 `reference`）
  - `search_references: Optional[List[SearchReference]]`：结构化引用（站点、标题、URL、摘要），主要方便前端或调试用

`DeepSearch.ResultsSummary.to_plaintext()` 就是这样把所有结果转成文本的：

```49:55:/home/roy/ubuntu/work/ai-app-lab/demohouse/deep_search/deep_search.py
    def to_plaintext(self) -> str:
        output = ""

        for key, value in self.ref_dict.items():
            output += f"\n【查询 “{key}” 得到的相关资料】"
            output += "\n".join([v.summary_content for v in value])

        return output
```

> **结论**：你新增的搜索引擎，**最关键是生成好的 `summary_content` 文本**，这样 LLM 才能吃到。

---

### 现有两个搜索引擎是怎么实现 & 调 AI

#### 1. TavilySearchEngine（第三方搜索 API）

文件：`search_engine/tavily.py`

- **构造参数（你可以看成是「搜索引擎配置」）**：
  - `api_key: str`：Tavily 的 API key
  - `search_depth: "basic" | "advanced"`
  - `topic: "general" | "news"`
  - `days: int`：多久以内的内容
  - `max_results: int`：最多返回多少条
  - `include_domains: Optional[str]`
  - `exclude_domains: Optional[str]`
- **核心调用外部 API 的地方**：

```56:65:/home/roy/ubuntu/work/ai-app-lab/demohouse/deep_search/search_engine/tavily.py
    def _search_single(self, query: str) -> SearchResult:
        response = self._tavily_client.search(
            query=query,
            search_depth=self._search_depth,
            topic=self._topic,
            days=self._days,
            max_results=self._max_results,
            include_domains=self._include_domains,
            exclude_domains=self._exclude_domains,
        )
        return SearchResult(
            query=query,
            summary_content=self._format_result(response),
        )
```

- **传给 LLM 的参数长什么样？**
  - Tavily 返回的是一堆 `results`，其中每条有 `title`、`content` 等。
  - `_format_result` 把它格式化成 **纯文本**：

```71:80:/home/roy/ubuntu/work/ai-app-lab/demohouse/deep_search/search_engine/tavily.py
    @classmethod
    def _format_result(cls, tavily_result: dict) -> str:
        results = tavily_result.get("results", [])
        formatted: str = ""
        for (i, result) in enumerate(results):
            formatted += f"参考资料{i + 1}: \n"
            formatted += f"标题: {result.get('title', '')}\n"
            formatted += f"内容: {result.get('content', '')}\n"
            formatted += "\n"
        return formatted
```

  - 这个 `formatted` 就是 `SearchResult.summary_content`，再经由 `ResultsSummary.to_plaintext()` 拼到一起，作为 `reference=...` 传给 **规划 LLM / 总结 LLM**：

```167:172:/home/roy/ubuntu/work/ai-app-lab/demohouse/deep_search/deep_search.py
            stream = llm.astream(
                reference=references.to_plaintext(),  # pass the search result to prompt template
                question=question,
                max_search_words=self.extra_config.max_search_words,
                meta_info=f"当前时间：{get_current_date()}"
            )
```

#### 2. VolcBotSearchEngine（有搜索插件的火山方舟 Bot）

文件：`search_engine/volc_bot.py`

- **构造参数**：
  - `bot_id: str`：Ark 上事先配置好的「带搜索插件」的 Bot id
  - `api_key: Optional[str]`：调用 Ark API 的 key（可选）
- **调用 Ark Runtime（真正「带搜索能力」的 AI 模型）的地方**：

```51:61:/home/roy/ubuntu/work/ai-app-lab/demohouse/deep_search/search_engine/volc_bot.py
    async def _run_bot_search(self, query: str) -> BotChatCompletion:
        return await self._ark_client.bot_chat.completions.create(
            messages=[
                {
                    "role": "user",
                    "content": query,
                }
            ],
            model=self._bot_id,
            stream=False,
        )
```

- 传给 AI（Ark Bot）的关键参数就是：
  - **`messages`**：标准 Chat 格式，role=`"user"`，content=当前搜索 query
  - **`model`**：这里就是 `bot_id`，代表「哪个 Bot（含搜索插件）」
  - **`stream`**：是否流式，这里是 `False`
- **把 AI 的返回结果转成 SearchResult：**

```63:75:/home/roy/ubuntu/work/ai-app-lab/demohouse/deep_search/search_engine/volc_bot.py
    @classmethod
    def _format_result(cls, response: BotChatCompletion, query: str) -> SearchResult:
        return SearchResult(
            query=query,
            summary_content=response.choices[0].message.content,
            search_references=[
                SearchReference(
                    site=r.site_name,
                    url=r.url,
                    content=r.summary,
                    title=r.title,
                ) for r in response.references if response.references
            ]
        )
```

- `summary_content`：Bot 的自然语言回答（已经是整合好的搜索结果）
- `search_references`：Bot 返回的 `references` 中的站点、URL、摘要等，结构化保存

> 这里你可以理解为：**VolcBotSearchEngine 把「搜索+整理解读」都交给了 Ark Bot 本身，然后只把 Bot 的回答当作 `summary_content` 提供给上层 LLM。**

---

### DeepSearch / DeepResearch 最终给 AI 的参数

你问的「需要使用到哪些参数来传给 ai」，可以分成两层：

#### A. 给「搜索 AI / 搜索服务」的参数

- **TavilySearchEngine → Tavily API**
  - `query`
  - `search_depth`
  - `topic`
  - `days`
  - `max_results`
  - `include_domains`
  - `exclude_domains`
- **VolcBotSearchEngine → Ark Bot（带搜索插件的 LLM）**
  - `messages=[{"role": "user", "content": query}]`
  - `model=self._bot_id`
  - `stream=False`
  - （`api_key` 在 client 初始化时传入）

#### B. 给「规划/总结 LLM（推理模型）」的参数（统一由 DeepSearch 处理）

在 `DeepSearch` 中，无论你用哪种搜索引擎，最后调用 `BaseChatLanguageModel` 时都是这些关键参数：

- `endpoint_id=self.planning_endpoint_id` / `self.summary_endpoint_id`（对应 config 里的推理模型）
- `messages=request.messages`（原始聊天上下文）
- 额外关键字参数：
  - **`reference=references.to_plaintext()`**：**所有搜索结果整理后的纯文本**（就是你 SearchEngine 产出的 `summary_content` 的汇总）
  - `question=question`：本次核心问题
  - `max_search_words`：每轮规划最多生成多少搜索关键词
  - `meta_info=f"当前时间：{get_current_date()}"`

---

### 如果你要新增一个搜索引擎，需要做什么？

1. **新建一个类继承 `SearchEngine`**
   - 在 `search_engine/` 目录，例如 `my_engine.py`：
   - 实现：
     - `def search(self, queries: List[str]) -> List[SearchResult]`
     - `async def asearch(self, queries: List[str]) -> List[SearchResult]`
2. **在实现中：**
   - 对每个 `query` 调用你自己的外部服务 / AI 接口（例如某个 HTTP 搜索 API，或另一个带检索插件的 LLM）
   - 把返回结果整理成：
     - `summary_content`：让 LLM好理解的「带标题+内容」的中文/英文文本
     - （可选）`search_references`：如果你的服务有 URL / 标题，就填到 `SearchReference` 里
3. **在入口 (`server.py` / `run_local.py`) 里根据配置选择你的新引擎**
   - 类似现在的：

```python
if SEARCH_ENGINE == "my_engine":
    search_engine = MySearchEngine(api_key=..., xxx=...)
```

---

### 小结（用一句话概括）

- **搜索引擎的职责**：对一组 `queries` 调用任何你想用的搜索 / AI 服务，把结果整理成 `SearchResult(summary_content=...)`。
- **传给上层 AI（推理/总结模型）的关键参数**：就是 `references.to_plaintext()` 这个「所有搜索结果拼接后的纯文本」，加上原始问题 `question` 和聊天 `messages`。

如果你接下来想「新增一个基于某个 HTTP 搜索 API 或 OpenAI 的搜索引擎」，我可以按上面接口帮你写一个完整的 `MySearchEngine` 模板代码。你更倾向基于哪种服务？